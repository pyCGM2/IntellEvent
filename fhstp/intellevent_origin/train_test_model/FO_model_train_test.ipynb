{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f1ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import os\n",
    "import copy\n",
    "import subprocess\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# utils.py imports\n",
    "from utils import get_train_test_split, reshape_data, get_sample_weights\n",
    "from utils import get_compiled_model #, eval_tr_model, eval_GS_v2, eval_te_data\n",
    "from utils import eval_fo_te_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function set_seed:\n",
    "    #Description:\n",
    "        #The set_seed function sets the seed for random number generators in the numpy and tensorflow libraries. \n",
    "        #It takes an integer value as an argument that is used as the seed. The function sets the seed for the following \n",
    "        #random number generators:\n",
    "\n",
    "        #    numpy.random.seed(): sets the seed for numpy random number generator.\n",
    "        #    tf.random.set_seed(): sets the seed for tensorflow random number generator.\n",
    "        #    tf.keras.seed: sets the seed for the keras module in tensorflow.\n",
    "        #    os.environ['PYTHONHASHSEED']: sets the seed for python's hash function.\n",
    "        #    os.environ['TF_DETERMINISTIC_OPS']: sets the flag to enforce deterministic behavior in tensorflow operations.\n",
    "\n",
    "    # This function is useful for ensuring reproducibility of the results generated by machine learning models \n",
    "    # that use random number generators. By setting the seed, the same set of random numbers will be generated \n",
    "    # every time the model is run, ensuring that the results are consistent.\n",
    "\n",
    "def set_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # Python general\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.keras.seed = seed\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4758d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM - Parameters\n",
    "depths = [100, 200, 300]\n",
    "layers = [2, 3]\n",
    "drop_out_vals = [0.2, 0.4, 0.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2094d88c",
   "metadata": {},
   "source": [
    "# Data Import for training and testing\n",
    "Note: the following description is how we implemented the code and data structure. Changes in this structure need to be applied in the code as well.\n",
    "\n",
    "Data is imported using a pandas Dataframe which contains trajectory information and events extracted from ground reaction force (GRF) plates. The table is organized as follows (Note: column names are also used in the utils.py file - so when changing column names, also check the utils.py file!):\n",
    "\n",
    "### 'Trial': \n",
    "    The name of each trial, represented as a string/char (e.g. 'DynamicA031.c3d').\n",
    "\n",
    "### 'label':\n",
    "    The label as an identifier for different pathologies, represented as an integer (e.g. 1,2,3, ... healthy, ICP, ...).\n",
    "    \n",
    "### 'DBid':\t\n",
    "    The unique identifier for each patient, represented as an integer.\n",
    "\n",
    "### 'UsDat':\t\n",
    "    The capture date of the trial, represented as a datetime object.\n",
    "    Note: with 'Trial', 'DBid', and 'USDat' each patient can be uniquely identified, even when there are more sessions.\n",
    "    \n",
    "### 'TR_TRAJ':\n",
    "    A 2-dimensional numpy array (features, num_frames) with 36 rows and a varying number of columns \n",
    "    (depending on the length of the trial). \n",
    "    The first 18 columns represent a trajectory in the three dimensions (X, Y, Z) of a certain marker (HEEL, TOE, ANKLE). \n",
    "    Column 1-6 are the X-trajectories of the left HEEL, TOE, and ANKLE marker and X-trajectories of the right HEEL, TOE, \n",
    "    and ANKLE marker. Columns 7-13 contain the Y-trajectories and the remaining columns contain the Z-trajectories \n",
    "    (in the same order as before). 'TR_TRAJ' is also the input for the training of the neural network.\n",
    "    The last 18 columns are the first derivaties of the above mentioned trajectories which represents \n",
    "    the velocity for each marker and axis. This leads to 36 features overall.\n",
    "    The length before the first and after the last event should be randomized for each trial, so that the neural network\n",
    "    learns different starting and ending points. In our paper we decided to use random values between 25 and 125. \n",
    "\n",
    "### 'TR_GRF_EVENTS':\n",
    "    A 1-dimensional numpy array (num_frames) representing the ground truth for training. Each element in the array \n",
    "    corresponds to a time step and contains one of the following values: 0 (no event), 1 (left foot off), or \n",
    "    2 (right foot off). This array only contains events captured by a GRF plates (ground truth data). \n",
    "    'TR_GRF_EVENTS' is also the input for the training of the neural network.\n",
    "    \n",
    "### 'TE_TRAJ':\n",
    "    A 2-dimensional numpy array (features, num_frames) that contains the same trajectories as 'TR_TRAJ', \n",
    "    but contains trajectories from the whole gait trial and not only for the GRF plate hits. 'TE_TRAJ' is used for \n",
    "    evaluating the performance of the beforehand trained model.\n",
    "    \n",
    "### 'TE_GRF_EVENTS': \n",
    "    A 1-dimensional numpy array (num_frames) that is the same length as 'TE_TRAJ' and contains only events from GRF\n",
    "    plate hits. 'TE_GRF_EVENTS' is used to identify the performance of the beforehand trained model on\n",
    "    ground truth data (=GRF plate hits). Note: our dataset was create with 0 = no event, 1 = left IC, 2 = left FO,\n",
    "    3 = right IC, and 4 = right FO! This is important, because based on these labels, the validation is programmed.  \n",
    "    \n",
    "\n",
    "### #ALL_EVENTS':\n",
    "    A 1-dimensional numpy array (num_frames) that contains is the same length as 'TE_TRAJ', but includes all events for\n",
    "    the whole gait trial, including events not only from GRF plates. \n",
    "    Note: our dataset was create with 0 = no event, 1 = left IC, 2 = left FO, 3 = right IC, and 4 = right FO!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e39687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imiport your DataFrame\n",
    "oss_data = # ... import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "oss_data = read_mat('/home/projectdrive/nextcloud/p_2019_o3dga/OSS_DATA_random_start_end_v3_5.mat')\n",
    "oss_data = pd.DataFrame(oss_data['OSS_DATA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa6bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a copy of the original data without a link\n",
    "oss = pd.DataFrame(columns = oss_data.columns, data = copy.deepcopy(oss_data.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb7a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further function description can be found in 'utils.py': \n",
    "# This function is used to split a given dataset into train, test, and validation sets. \n",
    "# It splits the data based on the labels present in the dataset and assigns a certain proportion of data to each set.\n",
    "# Stratifies patients based on Trial ID. All trials from each patient are either in the training, validation or test split.\n",
    "    # tr_data: This DataFrame contains the training data.\n",
    "    # te_data: This DataFrame contains the test data.\n",
    "    # tr_val_data: This DataFrame contains the training data for the validation set.\n",
    "    # te_val_data: This DataFrame contains the test data for the validation set.\n",
    "set_seed(5489) # Reproducibility!\n",
    "hold_out = 0.3  # test split size\n",
    "validation = 0.1 # validation split size\n",
    "tr_data, te_data, tr_val_data, te_val_data = get_train_test_split(oss, hold_out, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2495eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further function description in 'utils.py':\n",
    "# This function reshapes and pads sequences of trajectory and GRF data to the longest input sequence,\n",
    "# so that they can be used as input for the neural network. The input shape of tr_traj is (features, num_frames).\n",
    "# The input shape of tr_grf is (num_frames).\n",
    "# Specifically, it pads the sequences with zeros so that all sequences have the same length \n",
    "# (the length of the longest sequence), and it reshapes the data to have the shape \n",
    "# (num_samples, max_seq_length, num_features) for tr_traj and (num_samples, num_frames) for tr_grf, \n",
    "# as this is the input format for the neural network.  \n",
    "\n",
    "tr_val_traj, tr_val_grf = reshape_data(tr_val_data['TR_TRAJ'], tr_val_data['TR_GRF_EVENTS'])\n",
    "val_traj, val_grf = reshape_data( te_val_data['TR_TRAJ'], te_val_data['TR_GRF_EVENTS'])\n",
    "te_val_traj, te_val_grf = reshape_data(te_val_data['TE_TRAJ'], te_val_data['TE_GRF_EVENTS'])\n",
    "te_traj, te_grf = reshape_data(te_data['TE_TRAJ'], te_data['TE_GRF_EVENTS']) # or te_data['ALL_EVENTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18059a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of hyperparameters and early stopping meachenisms.\n",
    "# Note: these are parameters from our best approach. They might not work for your data.\n",
    "# 'min delta' depends a bit on the 'sample_weights', which drastically changes the 'val_loss' \n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=30, mode='auto', \n",
    "                   restore_best_weights=True, verbose=1, baseline=None)\n",
    "\n",
    "# num of input features\n",
    "features = 36\n",
    "\n",
    "# num of output nodes\n",
    "dense_out = 3\n",
    "\n",
    "# num of hidden units of the LSTM layer\n",
    "depth = 200\n",
    "\n",
    "# num of stacked LSTM layers\n",
    "layer = 3\n",
    "\n",
    "# Droput layer rate in a range [0-1]\n",
    "d_o_v = 0.4\n",
    "\n",
    "# samples are HIGHLY imbalanced between no-events and events, therefore we use sample weights to \n",
    "# give more weight to rarely-seen events. A ratio of 1:10 (0.1 to 1) was found to be the best ratio.\n",
    "weights = [\n",
    "            [0.1, 1, 1, 1, 1],\n",
    "            [0.01, 1, 1, 1, 1],\n",
    "            [0.001, 1, 1, 1, 1]\n",
    "          ]\n",
    "weight = weights[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284dfedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "set_seed(5489)  # REPRODUCIBILITY\n",
    "\n",
    "# Further function description in 'utils.py'.\n",
    "# each frame of tr_val_grf is assigned a sample weight based on the label (no-event: 0, initial contact: 1, ...)\n",
    "sample_weights = get_sample_weights(tr_val_grf, weight)\n",
    "\n",
    "# Further function description in 'utils.py'\n",
    "# creates the neural network with the given inputs (features, hidden units, dropout, layers, output nodes) \n",
    "# and compiles the model for training.\n",
    "model = get_compiled_model(features, depth, d_o_v, layer, dense_out)\n",
    "\n",
    "# Fit the model with the tr_val_traj and tr_val_grf data and validate eacht batch on the\n",
    "# val_traj and val_grf data for performance (underfitting and overfitting) reasons.\n",
    "history = model.fit(\n",
    "    tr_val_traj,\n",
    "    tf.one_hot(tr_val_grf, depth=3),\n",
    "    epochs=200, batch_size=64, verbose=1,\n",
    "    validation_data=(val_traj,tf.one_hot(val_grf, depth=3)),\n",
    "    callbacks=[es],\n",
    "    sample_weight=sample_weights,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c57b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the val_loss (underfitting/overfitting)\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39355fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This saves the model to the workspace\n",
    "model.save('FO_production_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5481be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function can import a certain model and check for example the summmary()\n",
    "model = tf.keras.models.load_model('FO_production_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7264dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further function description in 'utils.py':\n",
    "# The function evaluates the performance of the input model on the test dataset. \n",
    "# It calculates the mean absolute error for each event between prediction and GRF events \n",
    "# for each pathology separately (labels).\n",
    "# Note: This should only be used to validate the FINAL model!\n",
    "# this can be used for the validation set to find the best hyperparameters in a grid search or\n",
    "# Bayesian Optimization using the KerasTuner: https://keras.io/keras_tuner/\n",
    "\n",
    "fo_mae, fo_list_all, foclassification = eval_fo_te_data(model, te_traj, te_grf, te_data['label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
